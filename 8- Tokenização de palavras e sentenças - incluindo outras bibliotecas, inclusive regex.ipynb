{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hora de começar com o processamento de linguagem natural.', 'Python vai facilitar nossa vida!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "# Processo de dividir uma string em listas de pedaços ou \"tokens\". \n",
    "# Um token é uma parte inteira. Por exemplo: uma palavra é um token em uma sentença. Uma sentença é um token em um parágrafo.\n",
    "\n",
    "# Imports\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Texto\n",
    "frase = \"Hora de começar com o processamento de linguagem natural. Python vai facilitar nossa vida!\"\n",
    "\n",
    "# Tokenization em sentenças\n",
    "sent_tokens = sent_tokenize(frase)\n",
    "print(sent_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hora', 'de', 'começar', 'com', 'o', 'processamento', 'de', 'linguagem', 'natural', '.', 'Python', 'vai', 'facilitar', 'nossa', 'vida', '!']\n",
      "['ca', \"n't\"]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization em palavras\n",
    "word_tokens = word_tokenize(frase)\n",
    "print(word_tokens)\n",
    "print(word_tokenize(\"can't\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '.']\n"
     ]
    }
   ],
   "source": [
    "# Usando tokenizers customizados\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize('Hello World.'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can', \"'\", 't', 'is', 'a', 'contraction', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization por Pontuação\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(\"Can't is a contraction.\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Can't\", 'is', 'a', 'contraction']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization por expressões regulares\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "print(tokenizer.tokenize(\"Can't is a contraction.\"))\n",
    "\n",
    "# neste caso ele vai valorizar a contração do can't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Can't\", 'is', 'a', 'contraction']\n",
      "[\"Can't\", 'is', 'a', 'contraction.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization por expressões regulares\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "# regexp_tokenize\n",
    "print(regexp_tokenize(\"Can't is a contraction.\", \"[\\w']+\"))\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps = True)\n",
    "print(tokenizer.tokenize(\"Can't is a contraction.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
